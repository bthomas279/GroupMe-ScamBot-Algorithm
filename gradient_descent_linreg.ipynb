{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f869c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will show you how to automate the gradient descent process\n",
    "#This applies for a single variable\n",
    "import math, copy\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd1d9331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#This is for example data\\n#Example data\\nx_train = np.array([1.0, 2.0])   #features (column data used when referencing a CSV)\\ny_train = np.array([300.0, 500.0])   #target value\\n\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#This is for example data\n",
    "#Example data\n",
    "x_train = np.array([1.0, 2.0])   #features (column data used when referencing a CSV)\n",
    "y_train = np.array([300.0, 500.0])   #target value\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTICE: This block is only meant to test CSVs on these functions. Turn off if not currently\n",
    "#being used.\n",
    "\n",
    "cah = Table.read_table(\"academic_drop.csv\")\n",
    "\n",
    "def create_label(target_response):\n",
    "    if target_response == \"Dropout\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "target_dummy = cah.apply(create_label, \"Target\")\n",
    "\n",
    "target_dummy_label = cah.with_column(\"target\", target_dummy)\n",
    "\n",
    "#We're only using one variable and the binary target\n",
    "clean_data = target_dummy_label.select(\"Curricular units 1st sem (grade)\", \"target\")\n",
    "\n",
    "target_data = clean_data.select(\"target\")\n",
    "var_data = clean_data.select(\"Curricular units 1st sem (grade)\")\n",
    "\n",
    "x_train = clean_data.column(\"Curricular units 1st sem (grade)\")\n",
    "y_train = clean_data.column(\"target\")\n",
    "\n",
    "#This code functions correctly. Passed test. Can be used as reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99aaa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m)\n",
    "\n",
    "    return total_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To implement the gradient descent algorithm, you need three functions: \n",
    "#compute_gradient:Implementing the two equations gradient is defined as.\n",
    "#compute_cost: Measuring the cost over all of the training samples.\n",
    "#gradient_descent: Ultilizing compute_gradient and compute_cost to use gradient descent.\n",
    "\n",
    "#NOTICE: Reference Course 1 lab 4 and 6 to see the equations the functions are using\n",
    "\n",
    "def compute_gradient(x, y, w, b,):\n",
    "    \"\"\"This function computes the gradient for linear regression.\n",
    "    Args:\n",
    "        x: Data, m examples or variables\n",
    "        y: target values (target variable)\n",
    "        w,b: model parameters\n",
    "\n",
    "    Returns:\n",
    "        dj_dw: The gradient of the cost w.r.t. the parameters of w\n",
    "        dj_db: The gradient of the cost w.r.t the parameters of b\n",
    "    \"\"\"\n",
    "\n",
    "    #The number of training examples\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69100c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"This function performs gradient descent to fit w,b. Updates w,b by taking\n",
    "    num_iters gradient steps with learning rate alpha.\n",
    "\n",
    "    Args:\n",
    "        x: Data, m examples \n",
    "        y: target values\n",
    "        w_in,b_in: initial values of model parameters  \n",
    "        alpha: Learning rate\n",
    "        num_iters: number of iterations to run gradient descent\n",
    "        cost_function: function to call to produce cost\n",
    "        gradient_function: function to call to produce gradient\n",
    "    \n",
    "    Returns:\n",
    "        w (scalar): Updated value of parameter after running gradient descent\n",
    "        b (scalar): Updated value of parameter after running gradient descent\n",
    "        J_history (List): History of cost values\n",
    "        p_history (list): History of parameters [w,b] \n",
    "    \"\"\"\n",
    "\n",
    "    #Arrays to store cst J and w's at each iteration (used for graphing)\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        #Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)\n",
    "\n",
    "        #Update parameters using the gradient equation (3)\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        #Save cost J at each iteration\n",
    "        if i<100000: #Resource exhaustion prevention\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w,b])\n",
    "\n",
    "        #Print the cost at intervals 10 times or as many iterations if < 10\n",
    "        if i%  math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "            \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd91b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.13e-04  dj_dw: -2.331e+00, dj_db: -3.212e-01   w:  2.331e-02, b: 3.21203e-03\n",
      "Iteration 1000: Cost 1.13e-04  dj_dw:  1.961e-03, dj_db: -2.515e-02   w: -3.487e-02, b: 6.67119e-01\n",
      "Iteration 2000: Cost 1.13e-04  dj_dw:  3.556e-04, dj_db: -4.562e-03   w: -4.426e-02, b: 7.87626e-01\n",
      "Iteration 3000: Cost 1.13e-04  dj_dw:  6.450e-05, dj_db: -8.275e-04   w: -4.597e-02, b: 8.09484e-01\n",
      "Iteration 4000: Cost 1.13e-04  dj_dw:  1.170e-05, dj_db: -1.501e-04   w: -4.627e-02, b: 8.13448e-01\n",
      "Iteration 5000: Cost 1.13e-04  dj_dw:  2.122e-06, dj_db: -2.722e-05   w: -4.633e-02, b: 8.14167e-01\n",
      "Iteration 6000: Cost 1.13e-04  dj_dw:  3.849e-07, dj_db: -4.937e-06   w: -4.634e-02, b: 8.14298e-01\n",
      "Iteration 7000: Cost 1.13e-04  dj_dw:  6.980e-08, dj_db: -8.955e-07   w: -4.634e-02, b: 8.14321e-01\n",
      "Iteration 8000: Cost 1.13e-04  dj_dw:  1.266e-08, dj_db: -1.624e-07   w: -4.634e-02, b: 8.14325e-01\n",
      "Iteration 9000: Cost 1.13e-04  dj_dw:  2.296e-09, dj_db: -2.946e-08   w: -4.634e-02, b: 8.14326e-01\n",
      "(w,b) found by gradient descent: ( -0.0463,  0.8143)\n"
     ]
    }
   ],
   "source": [
    "#Finally it's time to run gradient descent\n",
    "#intialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "#Set interation and tmp_alpha\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "#run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35052d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's test the gradient descent with data from a csv. Use the dropout csv with only one\n",
    "#variable and the target value to test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
